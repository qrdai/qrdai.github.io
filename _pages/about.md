---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
# About Me
I am currently a first-year CS PhD student at [the University of Chicago](https://computerscience.uchicago.edu/), advised by [Prof. Chenhao Tan](https://chenhaot.com/). I am a member of the [Chicago Human+AI (CHAI)](https://chicagohai.github.io/) lab, and affiliated with the broader [Communication and Intelligence (C&I)](https://ci.cs.uchicago.edu/) group. I have also been working closely with [Prof. Hao Peng](https://haopeng-nlp.github.io/) at [the University of Illinois at Urbana-Champaign (UIUC)](https://cs.illinois.edu/). 

Previously, I received my Bachelor’s degree in Artificial Intelligence from [Fudan University](https://www.fudan.edu.cn/en/) in 2025. During my undergraduate study, I was fortunate to intern at UIUC with [Prof. Hao Peng](https://haopeng-nlp.github.io/) and [Prof. Jiaqi W. Ma](https://jiaqima.github.io/), and at Shanghai Jiao Tong University with [Prof. Dequan Wang](https://dequan.wang/).


<!-- <details>
<summary><strong>demo</strong></summary>

**Markdown** content here.

</details> -->



# Research Interests
I am broadly interested in post-training data and algorithms for large language models (LLMs). My current research focuses on the following specific topics:

- **Extending the generalization scope of reasoning and thinking behaviors of LLMs**. 
  I believe that genuine thoughtful reasoning should be a robust behavior that can transfer to versatile domains (e.g., philosophical and social science writing) and formalisms (e.g., Bayesian and causal reasoning), but the current math-centered post-training paradigm struggles towards this goal.

- **Evaluating and advancing complex and composite real-world capabilities of LLMs**.
  I am especially interested in training LLMs to (1) proactively explore and discover, (2) leverage ambiguity in strategic communications, and (3) balance accurate reasoning with controllable creativity.

In the past, I have mainly worked on data-centric topics of post-training, including data selection, data attribution, and data-efficient supervision paradigms. I still enjoy following the latest advances of these data-centric techniques, and am always excited about applying them to achieve my dynamic research goals.

<!-- <details>
<summary><strong>Extending the generalization scope of reasoning and thinking behaviors of LLMs.</strong></summary>
I believe that genuine thoughtful reasoning should be a robust behavior that can transfer to versatile domains (e.g., philosophical and social science writing) and formalisms (e.g., Bayesian and causal reasoning), but the current math-centered post-training paradigm struggles towards this goal.
</details>

<br>

<details>
<summary><strong>Evaluating and advancing complex and composite real-world capabilities of LLMs.</strong></summary>
I am especially interested in training LLMs to (1) proactively explore and discover, (2) leverage ambiguity in strategic communications, and (3) balance accurate reasoning with controllable creativity.
</details>

<br> -->



# Selected Publications

- **Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code**  
Aniket Vashishtha*, **Qirun Dai***, Hongyuan Mei, Amit Sharma, Chenhao Tan, Hao Peng  
*NeurIPS 2025 Workshop on Foundations of Reasoning in Language Models*  
[[paper]](https://arxiv.org/abs/2510.01539) [[code]](https://github.com/AniketVashishtha/Executable_Counterfactuals)

<!-- <br> -->

- **The Best Instruction-Tuning Data are Those That Fit**  
Dylan Zhang, **Qirun Dai**, Hao Peng  
*NeurIPS 2025 (Spotlight)*  
[[paper]](https://arxiv.org/abs/2502.04194)

<!-- <br> -->

- **Improving Influence-based Instruction Tuning Data Selection for Balanced Learning of Diverse Capabilities**  
**Qirun Dai**, Dylan Zhang, Jiaqi W Ma, Hao Peng  
*Findings of EMNLP 2025*  
[[paper]](https://arxiv.org/abs/2501.12147)



# News
**[12/2025]** Attending NeurIPS 2025 and will present my work, GRAPE (spotlight) and Executable Counterfactuals (FoRLM Workshop).

**[09/2025]** Officially started my CS PhD study as an honored member of the CHAI lab.

**[09/2025]** One paper accepted by EMNLP 2025 (Findings), and two papers by NeurIPS 2025 (Spotlight & FoRLM Workshop)!

**[06/2025]** Officially graduated from Fudan University and received my Bachelor’s degree.

**[04/2025]** Attending ICLR 2025 and will present my work, Balanced and Influential Data Selection (BIDS), at the 2nd DATA-FM Workshop.

<!-- [03/2025] Excited to join UChicago as an incoming CS PhD student. Honored to be part of the CHAI lab and the broader C&I group!

**[10/2024]** Attending COLM 2024. Excited to see both old and new friends!

**[08/2024]** Gave a [talk](https://trais-lab.github.io/dattri-reading-group/recording/2024/7/) on factual knowledge attribution in LLMs at the [data attribution reading group](https://trais-lab.github.io/dattri-reading-group/) hosted by Prof. Jiaqi W. Ma. -->